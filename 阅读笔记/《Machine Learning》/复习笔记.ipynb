{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TomMitchell:\n",
    "\n",
    "一个程序从经验E中学习，解决任务T，达到性能度量值P；当且仅当有了经验E后，经P评判，程序处理T时的性能有所提高。\n",
    "\n",
    "## 从单变量的线性回归开始\n",
    "$$特征的数量：1$$\n",
    "$$训练集中实例的数量:m$$\n",
    "$$特征/输入变量:x$$\n",
    "$$目标/输出变量:y$$\n",
    "$$训练集中的实例:(x,y)$$\n",
    "$$训练集中第i个实例:(x^{(i)},y^{(i)})$$\n",
    "$$函数/假设:h$$\n",
    "单变量线性回归问题：$$h_\\theta(x)=\\theta_0+\\theta_1x$$\n",
    "代价函数（建模误差）：$$J(\\theta_0,\\theta_1)=\\cfrac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "我们的目标是建立模型h，使得代价函数J最小，即：$$\\underbrace{minimize}_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$$\n",
    "### 批量梯度下降(同时更新2个theta)：$$\\theta_j:=\\theta_j-\\alpha\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)$$\n",
    "（更新theta的原理在于后面的导数项，通过斜率正负确保随着theta的增大或减小，J(theta）一定随之减小，同时因为J(theta)的斜率变化，theta的变化也随着J的减小而减小，如在接近局部最低点时导数接近0，此时梯度下降法会自动采取更小的幅度）。\n",
    "\n",
    "所以梯度下降的实现关键在于求出代价函数J的导数：$$\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)=\\cfrac{\\partial}{\\partial\\theta_j}\\cfrac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "j=0:$$\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)=\\cfrac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})$$\n",
    "j=1:$$\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)=\\cfrac{1}{2m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})·x^{(i)})$$\n",
    "### 正规方程\n",
    "通过求解方程来找出使J最小的参数(不可逆矩阵不可用)：\n",
    "$$\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_j)=0$$\n",
    "假设训练集的特征矩阵为X，训练集的结果为向量y，则：\n",
    "$$\\theta=(X^TX)^{(-1)}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T05:01:04.190311Z",
     "start_time": "2019-10-04T05:01:04.184126Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def normalEqn(X,y):\n",
    "    theta=np.linalg.inv(X.T@X)@X.T@y\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多维特征情形\n",
    "### 通用情形\n",
    "$$特征的数量：n$$\n",
    "$$训练集中的实例x^{(i)}成为行向量$$\n",
    "$$x_j^{(i)}是第i个训练实例的第j个特征$$\n",
    "$$X是类似m×n的矩阵，m是实例个数，n是特征个数$$\n",
    "多变量的假设：$$h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$$\n",
    "为了简化公式，引入$x_0=1$：\n",
    "$$h_\\theta(x)=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$$\n",
    "即：$$h_\\theta(x)=\\theta^T(X)$$\n",
    "我们的目标和单变量情形下一样，只不过theta更多：\n",
    "$$\\theta_0=\\theta_0-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})·x_0^{(i)})$$\n",
    "$$\\theta_1=\\theta_1-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})·x_1^{(i)})$$\n",
    "$$...$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T03:43:23.693223Z",
     "start_time": "2019-10-04T03:43:23.686333Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def computeCost(X,y,theta):\n",
    "    inner=np.power(((X*theta.T)-y),2)\n",
    "    return np.sum(inner)/(2*len(X))\n",
    "#len(X)代表m而不是n，注意！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-04T03:53:04.804Z"
    }
   },
   "source": [
    "### 优化\n",
    "#### 特征放缩\n",
    "当两个特征值范围差的很远，如0-5和0-2000时，代价函数的等高线图能看出图像很扁，梯度下降也需要非常多次迭代才能收敛，方法是尝试将所有特征的尺度都缩放到-1到1之间：$$x_n=\\cfrac{x_n-\\mu_n}{s_n}$$\n",
    "$$\\mu_n是平均值$$\n",
    "$$s_n是标准差$$\n",
    "#### 学习率\n",
    "梯度下降算法的迭代受到学习率alpha的影响，如果学习率过小，则达到收敛所需的迭代次数会非常高；如过大，则每次迭代可能不会减小代价函数，越过局部最小值而导致无法收敛。\n",
    "建议的学习率：\n",
    "$$\\alpha=0.01,0.03,0.1,0.3,1,3,10$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归\n",
    "引入模型，使得模型的输出变量范围始终在0和1之间，以便完成分类：\n",
    "$$h_\\theta(x)=g(\\theta^TX)$$\n",
    "X代表特征向量,g代表逻辑函数，此处为Sigmoid函数：\n",
    "$$g(z)=\\cfrac{1}{1+e^{-z}}$$\n",
    "故：\n",
    "$$h_\\theta(x)=\\cfrac{1}{1+e^{-\\theta^TX}}$$\n",
    "此时h的作用是对于给定的输入变量，输出变量=1的可能性，即：\n",
    "$$h_\\theta(x)=P(y=1|x;\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:00:03.680837Z",
     "start_time": "2019-10-04T07:00:03.674998Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们预测的规则是h>0.5则y=1,反之y=0；又因为g(z)的0.5分界在于z=0，所以$\\theta^Tx?0$是判断的边界。故决策边界（曲线）方程即是：\n",
    "$$\\theta^Tx=0$$\n",
    "线性回归的代价函数：\n",
    "$$J(\\theta)=\\cfrac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "逻辑回归的代价函数：\n",
    "$$J(\\theta)=\\cfrac{1}{m}\\sum_{i=1}^mCost(h_\\theta(x^{(i)})-y^{(i)})$$\n",
    "y=1时：\n",
    "$$Cost(h_\\theta(x),y)=-log(h_\\theta(x))$$\n",
    "y=0时：\n",
    "$$Cost(h_\\theta(x),y)=-log(1-h_\\theta(x))$$\n",
    "简化之：\n",
    "$$J(\\theta)=-\\cfrac{1}{m}\\sum_{i=1}^m[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:00:07.314005Z",
     "start_time": "2019-10-04T07:00:07.305905Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cost(theta,X,y):\n",
    "    theta=np.matrix(theta)\n",
    "    X=np.matrix(X)\n",
    "    y=np.matrix(y)\n",
    "    first=np.multiply(y,np.log(sigmoid(X*theta.T)))\n",
    "    second=np.multiply((1-y),mnp.log(1-sigmoid(X*theta.T)))\n",
    "    return np.sum(first+second)/-(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仍可以用梯度下降算法求使代价函数最小的参数：\n",
    "$$\\theta_j=\\theta_j-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})·x_j^{(i)})$$\n",
    "易知代价函数会带来一个凸优化问题，即$J(\\theta)$会是一个凸函数，并且没有局部最优值。\n",
    "\n",
    "除了梯度下降之外，常用来令代价函数最小的算法：\n",
    "* 共而梯度Conjugate Gradient\n",
    "* 局部优化法Broyden fletcher goldfarb shann,BFGS\n",
    "* 有限内存局部优化法LBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "面对过拟合问题，一是想办法减少特征量，二便是正则化（保留所有特征但是减小参数的大小）。\n",
    "\n",
    "过拟合来自于高次项（虚线过于扭曲），因而可以通过让高次项的系数接近于0来完成拟合。为此要在代价函数中为$\\theta$添加惩罚。\n",
    "\n",
    "此时的代价函数为：\n",
    "$$J(\\theta)=\\cfrac{1}{2m}[\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2+\\lambda\\sum_{j=1}^n\\theta_j^2]$$\n",
    "$\\lambda$即为正则化参数，过大会导致模型欠拟合，过小则导致所有$\\theta$都趋于0。\n",
    "\n",
    "### 正则化线性回归\n",
    "代价函数：\n",
    "$$J(\\theta)=\\cfrac{1}{2m}[\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2+\\lambda\\sum_{j=1}^n\\theta_j^2]$$\n",
    "梯度下降（未对$\\theta_0$正则化）：\n",
    "$$\\theta_0:=\\theta_0-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)}$$\n",
    "$$\\theta_j:=\\theta_j-\\alpha[\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\\cfrac{\\lambda}{m}\\theta_j]$$\n",
    "正则回归(矩阵尺寸为(n+1)*(n+1))：\n",
    "$$\\theta=(X^TX+\\lambda[0 1 1 ... 1])^{-1}X^Ty$$\n",
    "\n",
    "### 正则化逻辑回归\n",
    "代价函数：\n",
    "$$J(\\theta)=-\\cfrac{1}{m}\\sum_{i=1}^m[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]+\\cfrac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2$$\n",
    "梯度下降（未对$\\theta_0$正则化）：\n",
    "$$\\theta_0:=\\theta_0-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)}$$\n",
    "$$\\theta_j:=\\theta_j-\\alpha[\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\\cfrac{\\lambda}{m}\\theta_j]$$\n",
    "看上去和线性回归一样，但是$h_\\theta(x)=g(\\theta^TX)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "$$m个训练样本$$\n",
    "$$每个样本包含一组输入x和一组输出y$$\n",
    "$$L表示神经网络层数Layer$$\n",
    "$$S_I表示每层的神经元个数$$\n",
    "$$S_l表示输出层神经元个数$$\n",
    "$$S_L表示最后一层中处理单元的个数$$\n",
    "### 代价函数：\n",
    "$$J(\\theta)=-\\cfrac{1}{m}[\\sum_{i=1}^m\\sum_{k=1}^ky_k^{(i)}log(h_\\theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\\theta(x^{(i)}))_k)]+\\cfrac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\theta_{ji}^{(l)})^2$$\n",
    "### 反向传播算法\n",
    "为了计算代价函数的偏导数$\\cfrac{\\partial}{\\partial\\theta_{ij}^{(l)}}$,先计算最后一层的误差，再一层一层反向求出各层误差，直到倒数第二层。\n",
    "### * 亟待补充 \n",
    "### 使用步骤\n",
    "1.参数的随机初始化\n",
    "\n",
    "2.利用正向传播方法计算所有的$h_\\theta(x)$\n",
    "\n",
    "3.编写计算代价函数J的代码\n",
    "\n",
    "4.利用反向传播方法计算所有偏导数\n",
    "\n",
    "5.利用数值检验方法检验偏导数\n",
    "\n",
    "6.利用优化算法来最小化代价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
