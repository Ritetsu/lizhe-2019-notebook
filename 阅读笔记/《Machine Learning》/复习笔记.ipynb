{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TomMitchell:\n",
    "\n",
    "一个程序从经验E中学习，解决任务T，达到性能度量值P；当且仅当有了经验E后，经P评判，程序处理T时的性能有所提高。\n",
    "\n",
    "## 从单变量的线性回归开始\n",
    "特征的数量：$$1$$\n",
    "训练集中实例的数量:$$m$$\n",
    "特征/输入变量:$$x$$\n",
    "目标/输出变量:$$y$$\n",
    "训练集中的实例:$$(x,y)$$\n",
    "训练集中第i个实例:$$(x^{(i)},y^{(i)})$$\n",
    "函数/假设:$$h$$\n",
    "单变量线性回归问题：$$h_\\theta(x)=\\theta_0+\\theta_1x$$\n",
    "代价函数（建模误差）：$$J(\\theta_0,\\theta_1)=\\cfrac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "我们的目标是建立模型h，使得代价函数J最小，即：$$\\underbrace{minimize}_{\\theta_0,\\theta_1}J(\\theta_0,\\theta_1)$$\n",
    "### 批量梯度下降(同时更新2个theta)：$$\\theta_j:=\\theta_j-\\alpha\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)$$\n",
    "（更新theta的原理在于后面的导数项，通过斜率正负确保随着theta的增大或减小，J(theta）一定随之减小，同时因为J(theta)的斜率变化，theta的变化也随着J的减小而减小，如在接近局部最低点时导数接近0，此时梯度下降法会自动采取更小的幅度）。\n",
    "\n",
    "所以梯度下降的实现关键在于求出代价函数J的导数：$$\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)=\\cfrac{\\partial}{\\partial\\theta_j}\\cfrac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "j=0:$$\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)=\\cfrac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})$$\n",
    "j=1:$$\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)=\\cfrac{1}{2m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})·x^{(i)})$$\n",
    "### 正规方程\n",
    "通过求解方程来找出使J最小的参数(不可逆矩阵不可用)：\n",
    "$$\\cfrac{\\partial}{\\partial\\theta_j}J(\\theta_j)=0$$\n",
    "假设训练集的特征矩阵为X，训练集的结果为向量y，则：\n",
    "$$\\theta=(X^TX)^{(-1)}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T05:01:04.190311Z",
     "start_time": "2019-10-04T05:01:04.184126Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def normalEqn(X,y):\n",
    "    theta=np.linalg.inv(X.T@X)@X.T@y\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多维特征情形\n",
    "### 通用情形\n",
    "特征的数量：$$n$$\n",
    "训练集中的实例成为行向量$$x^{(i)}$$\n",
    "第i个训练实例的第j个特征$$x_j^{(i)}$$\n",
    "X是类似m×n的矩阵，m是实例个数，n是特征个数\n",
    "\n",
    "多变量的假设：$$h_\\theta(x)=\\theta_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$$\n",
    "为了简化公式，引入$x_0=1$：\n",
    "$$h_\\theta(x)=\\theta_0x_0+\\theta_1x_1+\\theta_2x_2+...+\\theta_nx_n$$\n",
    "即：$$h_\\theta(x)=\\theta^T(X)$$\n",
    "我们的目标和单变量情形下一样，只不过theta更多：\n",
    "$$\\theta_0=\\theta_0-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})·x_0^{(i)})$$\n",
    "$$\\theta_1=\\theta_1-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})·x_1^{(i)})$$\n",
    "$$...$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T03:43:23.693223Z",
     "start_time": "2019-10-04T03:43:23.686333Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def computeCost(X,y,theta):\n",
    "    inner=np.power(((X*theta.T)-y),2)\n",
    "    return np.sum(inner)/(2*len(X))\n",
    "#len(X)代表m而不是n，注意！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-04T03:53:04.804Z"
    }
   },
   "source": [
    "### 优化\n",
    "#### 特征放缩\n",
    "当两个特征值范围差的很远，如0-5和0-2000时，代价函数的等高线图能看出图像很扁，梯度下降也需要非常多次迭代才能收敛，方法是尝试将所有特征的尺度都缩放到-1到1之间：$$x_n=\\cfrac{x_n-\\mu_n}{s_n}$$\n",
    "平均值$\\mu_n$，标准差$s_n$\n",
    "#### 学习率\n",
    "梯度下降算法的迭代受到学习率alpha的影响，如果学习率过小，则达到收敛所需的迭代次数会非常高；如过大，则每次迭代可能不会减小代价函数，越过局部最小值而导致无法收敛。\n",
    "建议的学习率：\n",
    "$$\\alpha=0.01,0.03,0.1,0.3,1,3,10$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归\n",
    "引入模型，使得模型的输出变量范围始终在0和1之间，以便完成分类：\n",
    "$$h_\\theta(x)=g(\\theta^TX)$$\n",
    "X代表特征向量,g代表逻辑函数，此处为Sigmoid函数：\n",
    "$$g(z)=\\cfrac{1}{1+e^{-z}}$$\n",
    "故：\n",
    "$$h_\\theta(x)=\\cfrac{1}{1+e^{-\\theta^TX}}$$\n",
    "此时h的作用是对于给定的输入变量，输出变量=1的可能性，即：\n",
    "$$h_\\theta(x)=P(y=1|x;\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:00:03.680837Z",
     "start_time": "2019-10-04T07:00:03.674998Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们预测的规则是h>0.5则y=1,反之y=0；又因为g(z)的0.5分界在于z=0，所以$\\theta^Tx?0$是判断的边界。故决策边界（曲线）方程即是：\n",
    "$$\\theta^Tx=0$$\n",
    "线性回归的代价函数：\n",
    "$$J(\\theta)=\\cfrac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "逻辑回归的代价函数：\n",
    "$$J(\\theta)=\\cfrac{1}{m}\\sum_{i=1}^mCost(h_\\theta(x^{(i)})-y^{(i)})$$\n",
    "y=1时：\n",
    "$$Cost(h_\\theta(x),y)=-log(h_\\theta(x))$$\n",
    "y=0时：\n",
    "$$Cost(h_\\theta(x),y)=-log(1-h_\\theta(x))$$\n",
    "简化之：\n",
    "$$J(\\theta)=-\\cfrac{1}{m}\\sum_{i=1}^m[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T07:00:07.314005Z",
     "start_time": "2019-10-04T07:00:07.305905Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cost(theta,X,y):\n",
    "    theta=np.matrix(theta)\n",
    "    X=np.matrix(X)\n",
    "    y=np.matrix(y)\n",
    "    first=np.multiply(y,np.log(sigmoid(X*theta.T)))\n",
    "    second=np.multiply((1-y),mnp.log(1-sigmoid(X*theta.T)))\n",
    "    return np.sum(first+second)/-(len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "仍可以用梯度下降算法求使代价函数最小的参数：\n",
    "$$\\theta_j=\\theta_j-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})·x_j^{(i)})$$\n",
    "易知代价函数会带来一个凸优化问题，即$J(\\theta)$会是一个凸函数，并且没有局部最优值。\n",
    "\n",
    "除了梯度下降之外，常用来令代价函数最小的算法：\n",
    "* 共而梯度Conjugate Gradient\n",
    "* 局部优化法Broyden fletcher goldfarb shann,BFGS\n",
    "* 有限内存局部优化法LBFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "面对过拟合问题，一是想办法减少特征量，二便是正则化（保留所有特征但是减小参数的大小）。\n",
    "\n",
    "过拟合来自于高次项（虚线过于扭曲），因而可以通过让高次项的系数接近于0来完成拟合。为此要在代价函数中为$\\theta$添加惩罚。\n",
    "\n",
    "此时的代价函数为：\n",
    "$$J(\\theta)=\\cfrac{1}{2m}[\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2+\\lambda\\sum_{j=1}^n\\theta_j^2]$$\n",
    "$\\lambda$即为正则化参数，过大会导致模型欠拟合，过小则导致所有$\\theta$都趋于0。\n",
    "\n",
    "### 正则化线性回归\n",
    "代价函数：\n",
    "$$J(\\theta)=\\cfrac{1}{2m}[\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2+\\lambda\\sum_{j=1}^n\\theta_j^2]$$\n",
    "梯度下降（未对$\\theta_0$正则化）：\n",
    "$$\\theta_0:=\\theta_0-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)}$$\n",
    "$$\\theta_j:=\\theta_j-\\alpha[\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\\cfrac{\\lambda}{m}\\theta_j]$$\n",
    "正则回归(矩阵尺寸为(n+1)*(n+1))：\n",
    "$$\\theta=(X^TX+\\lambda[0 1 1 ... 1])^{-1}X^Ty$$\n",
    "\n",
    "### 正则化逻辑回归\n",
    "代价函数：\n",
    "$$J(\\theta)=-\\cfrac{1}{m}\\sum_{i=1}^m[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]+\\cfrac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2$$\n",
    "梯度下降（未对$\\theta_0$正则化）：\n",
    "$$\\theta_0:=\\theta_0-\\alpha\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})x_0^{(i)}$$\n",
    "$$\\theta_j:=\\theta_j-\\alpha[\\cfrac{1}{m}\\sum_{i=1}^m((h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\\cfrac{\\lambda}{m}\\theta_j]$$\n",
    "看上去和线性回归一样，但是$h_\\theta(x)=g(\\theta^TX)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络\n",
    "训练样本个数$$m$$\n",
    "每个样本包含一组输入和一组输出$$x，y$$\n",
    "表示神经网络层数Layer$$L$$\n",
    "表示每层的神经元个数$$S_I$$\n",
    "表示输出层神经元个数$$S_l$$\n",
    "表示最后一层中处理单元的个数$$S_L$$\n",
    "### 代价函数：\n",
    "$$J(\\theta)=-\\cfrac{1}{m}[\\sum_{i=1}^m\\sum_{k=1}^ky_k^{(i)}log(h_\\theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\\theta(x^{(i)}))_k)]+\\cfrac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\theta_{ji}^{(l)})^2$$\n",
    "### 反向传播算法\n",
    "为了计算代价函数的偏导数$\\cfrac{\\partial}{\\partial\\theta_{ij}^{(l)}}$,先计算最后一层的误差，再一层一层反向求出各层误差，直到倒数第二层。\n",
    "### * 亟待补充 \n",
    "### 使用步骤\n",
    "1.参数的随机初始化\n",
    "\n",
    "2.利用正向传播方法计算所有的$h_\\theta(x)$\n",
    "\n",
    "3.编写计算代价函数J的代码\n",
    "\n",
    "4.利用反向传播方法计算所有偏导数\n",
    "\n",
    "5.利用数值检验方法检验偏导数\n",
    "\n",
    "6.利用优化算法来最小化代价函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 应用建议\n",
    "当运用训练好了的模型预测未知数据时发现有较大误差，下一步？\n",
    "\n",
    "1.减少特征的数量；\n",
    "\n",
    "2.获取更多特征；\n",
    "\n",
    "3.增加多项式特征（如$x^2，x_1·x_2$等）\n",
    "\n",
    "4.减少or增加正则化程度$\\lambda$\n",
    "### 对假设的评估\n",
    "通常，数据=70%训练集+30%测试集；获得模型后，可以：\n",
    "\n",
    "1.线性回归：直接用测试集数据计算代价函数J；\n",
    "\n",
    "2.逻辑回归：计算代价函数外，还可计算错误率；\n",
    "### 模型选择和交叉验证集\n",
    "即，**数据=60%训练集+20%交叉验证集+20%测试集**；之后：\n",
    "\n",
    "1.使用**训练集**训练处n个模型；\n",
    "\n",
    "2.用n个模型分别对**交叉验证集**计算得出交叉验证误差（代价函数值）；\n",
    "\n",
    "3.选取代价函数值最小的模型；\n",
    "\n",
    "4.用3中选出的模型对**测试集**计算得出推广误差（代价函数值）。\n",
    "### 偏差和方差\n",
    "偏差较大→欠拟合；方差较大→过拟合。\n",
    "\n",
    "可以通过将训练集和交叉验证集的代价函数与**多项式的次数**绘制在同一张图表上来帮助分析，训练集代价函数肯定是随次数上升减小的，而交叉验证集是U形：\n",
    "\n",
    "二者相近：偏差/欠拟合\n",
    "\n",
    "交叉远大于训练：方差/过拟合\n",
    "\n",
    "而**正则化系数$\\lambda$也**有所贡献，训练集误差随着$\\lambda$而增加，交叉验证集则随$\\lambda$增加先减小后增加；因为$\\lambda$增加使得$\\theta$的贡献降低了：\n",
    "\n",
    "**学习曲线**\n",
    "\n",
    "将训练集误差和交叉验证集误差作为训练集实例数量（m）函数而绘制的图→说明高方差/过拟合情况下增加更多数据可能提升算法效果。\n",
    "\n",
    "### 建议Solution\n",
    "1.获取更多的数据（训练实例）→高方差√\n",
    "\n",
    "2.减少特征数量→高方差√\n",
    "\n",
    "3.增加特征数量→高偏差√\n",
    "\n",
    "4.增加多项式特征→高偏差√\n",
    "\n",
    "5.减少正则化程度$\\lambda$→高偏差√\n",
    "\n",
    "6.增加正则化程度$\\lambda$→高方差√\n",
    "\n",
    "### 类偏斜\n",
    "一个极端的例子，假设在用算法预测癌症是良性还是恶性，那么如果说训练集的样本中只有0.5%是恶性，其余全是良性；那么我们平白无故算法A预测所有肿瘤都是良性，误差为0.5%；而写出来的神经网络算法B训练后误差可能有1%，这个时候显然不能根据误差判断A比B算法好。\n",
    "\n",
    "**查准率precision=TP/(TP+FP)**，即所有预测有恶性肿瘤的病人中，实际有恶性肿瘤的百分比，越↑越好。\n",
    "\n",
    "**查全率recall=TP/(TP+FN)**，即所有实际有恶性肿瘤的病人中，被预测到的百分比，越↑越好。\n",
    "\n",
    "如何权衡这两个指标呢？根据需求！\n",
    "\n",
    "可以在不同阈值的情况下，将二者关系绘制成图表（x/y轴是二者）；一个帮助选择阈值的方法：**F1值法**：\n",
    "$$F1=\\cfrac{2PR}{P+R}$$\n",
    "选择使F1最高的阈值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机\n",
    "### 从逻辑回归开始\n",
    "逻辑回归的假设：\n",
    "$$h_\\theta(x)=\\cfrac{1}{1+e^{-\\theta^Tx}}$$\n",
    "若y=1，我们希望$h_\\theta(x)\\approx1$，$\\theta^Tx\\gg0$\n",
    "\n",
    "若y=0，则$h_\\theta(x)\\approx0$，$\\theta^Tx\\ll0$（注意考虑S型的函数）\n",
    "\n",
    "考虑到代价函数：\n",
    "$$J(\\theta)=-\\cfrac{1}{m}\\sum_{i=1}^m[y^{(i)}log(h_\\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$$\n",
    "\n",
    "y=0或y=1时代价函数会消掉一部分，导致$\\theta^Tx$极大或极小能确保代价函数趋于0。\n",
    "\n",
    "据此构建支持向量机：\n",
    "$$\\underbrace{min}_{\\theta}C\\sum^{m}_{i=1}[y^{(i)}cost_1(\\theta^Tx^{(i)}+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]+\\cfrac{1}{2}\\sum_{j=1}^{n}\\theta_j^2$$\n",
    "\n",
    "此处的C，相当于之前的$\\cfrac{1}{\\lambda}$，因此：\n",
    "\n",
    "C较大时，相当于$\\lambda$较小，可能导致过拟合，高方差；\n",
    "\n",
    "C较小时，相当于$\\lambda$较大，可能导致低拟合，高偏差。\n",
    "\n",
    "向量u的范数=$\\sqrt{u_1^2+u_2^2}$，代表u的欧几里得长度。\n",
    "\n",
    "通过向量内积：\n",
    "$$\\vec u^T\\vec v=\\vec v^T\\vec u=||u||p$$\n",
    "注意p和$||u||$都是实数，p作为投影是有方向的（正负）。\n",
    "\n",
    "从而实现：\n",
    "$$\\theta^Tx^{(i)}=p^{(i)}·||\\theta||$$\n",
    "\n",
    "### 核函数\n",
    "如果说模型是：\n",
    "$$\\theta_0+\\theta_1x_1+\\theta_2x_2+...\\theta_4x1^2+...$$\n",
    "那么我们可以用一系列新的特征f来替换模型中的每一项，如：\n",
    "$$f_1=x_1,f_2=x_2,f_3=x_1^2...$$\n",
    "然而除了对原有特征进行组合以外，我们还可以用**核函数**来构建新的特征。\n",
    "\n",
    "给定一个训练实例x，我们可以利用x的各个特征与我们预先选定的landmarks，$l^{1},l^{(2)},l^{(3)}$的近似程度来选新的特征$f_1,f_2,f_3$，例如：\n",
    "$$f_1=similarity(x,l^{(1)})=e(-\\cfrac{||x-l^{(1)}||^2}{2\\sigma^2})$$\n",
    "其中：\n",
    "$$||x-l^{(1)}||^2=\\sum_{j=1}^{n}(x_j-l_j^{(1)})^2$$\n",
    "即x中的所有特征与$l^{(1)}$的距离之和，上面的similarity就是一个高斯核函数。\n",
    "\n",
    "如果x与l之间距离近似于0，则新特征f→1；反之较远则f→0。\n",
    "\n",
    "（SVM也可以不使用核函数，不使用又称为线性核函数，一般训练集特征非常多而实例非常少的时候可以采用）\n",
    "\n",
    "下面是支持向量机的参数C（之前有）和$\\delta$的影响：\n",
    "\n",
    "1.$\\delta$较大时，可能导致低方差，高偏差；\n",
    "\n",
    "2.$\\delta$较小时，可能导致低偏差，高方差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无监督学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
